{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e8eb97",
   "metadata": {},
   "source": [
    "# Retail Sales Analysis Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0e0b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#All Models in this program test the validity of the model at predicting actiual values.  \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I have not yet added prediction/forecasting functionality.  I will do one week's work of prediction at a time.  \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#All Models in this program test the validity of the model at predicting actiual values.  \n",
    "# I have not yet added prediction/forecasting functionality.  I will do one week's work of prediction at a time.  \n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import statsmodels.api as sm\n",
    "import operator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_vectorized_v2(a, b): \n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the initial Data Frame from the potLog6 csv.  Data from 1/1/2020 to 3/31/2020\n",
    "\n",
    "df = pd.read_csv('csv/PotLog.csv')\n",
    "\n",
    "\n",
    "#creating the dictionary to hold the Errors of each method.  Will find minimum(best) value at the end\n",
    "rmseDictionary = {}\n",
    "\n",
    "mapeDictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearanging data into two new data frames aggregated by the sums of days\n",
    "#historic data contains the months of jan and february\n",
    "#test data is the month of march\n",
    "\n",
    "allData = df.copy()\n",
    "allData['Timestamp'] = pd.to_datetime(allData.date,format='%Y-%m-%d')\n",
    "allData.index = allData.Timestamp\n",
    "allData = allData.resample('D').sum()\n",
    "allData = allData.drop(columns=['hour', 'week'])\n",
    "\n",
    "historic = df[:720].copy()\n",
    "\n",
    "historic['Timestamp'] = pd.to_datetime(historic.date,format='%Y-%m-%d')\n",
    "historic.index = historic.Timestamp\n",
    "historic = historic.resample('D').sum()\n",
    "historic = historic.drop(columns=['hour', 'week'])\n",
    "\n",
    "test = df[720:].copy()\n",
    "\n",
    "test['Timestamp'] = pd.to_datetime(test.date,format='%Y-%m-%d')\n",
    "test.index = test.Timestamp\n",
    "test = test.resample('D').sum()\n",
    "test = test.drop(columns=['hour', 'week'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c982fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Historic and Test data on the same plane\n",
    "\n",
    "historic.sales.plot(figsize=(15,8), title= 'Sales', fontsize=14)\n",
    "test.sales.plot(figsize=(15,8), title= 'Sales', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94714bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd= np.asarray(allData.sales)\n",
    "y_hat_avg = allData.copy()\n",
    "y_hat_avg['naive'] = 0 \n",
    "#print (y_hat_avg['sales'][3])\n",
    "#print (len(y_hat_avg))\n",
    "\n",
    "y_hat_avg['naive'][0] = allData.sales[0]\n",
    "i = 1\n",
    "for index, row in y_hat_avg.iterrows():\n",
    "    if i < len(y_hat_avg):\n",
    "        y_hat_avg['naive'][i] = allData.sales[i-1]\n",
    "        #print(index)\n",
    "        #sale = row.sales\n",
    "        #print(sale)\n",
    "        i +=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "y_hat_avg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c127dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easy or naive forcasting method.  It predicts values according to the value of the previous day \n",
    "#This needs to be redone.  It shuold not be a straight line but rather a scatter plot\n",
    "\n",
    "#dd= np.asarray(allData.sales)\n",
    "#y_hat_avg = test.copy()\n",
    "#y_hat_avg['naive'] = dd[len(dd)-1]\n",
    "plt.figure(figsize=(12,8))\n",
    "#plt.plot(historic.index, historic['sales'], label='Historic Sales')\n",
    "plt.plot(allData.index,allData['sales'], label='Actual')\n",
    "plt.plot(y_hat_avg.index,y_hat_avg['naive'], label='Predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Naive Forecast\")\n",
    "plt.show()\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg.naive))\n",
    "rmseDictionary[\"Naive\"] = rmse\n",
    "print(\"The RMS for the Naive Method is equal to {}\".format(rmse))\n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg.naive)\n",
    "mapeDictionary['Naive'] = mape \n",
    "\n",
    "y_hat_avg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7700e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Simple Average forcasting method forcasts according the overall average of sales\n",
    "\n",
    "#y_hat_avg = test.copy()\n",
    "y_hat_avg['avg_forecast'] = allData['sales'].mean()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(historic['sales'], label='Historic')\n",
    "plt.plot(test['sales'], label='Test')\n",
    "plt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg.avg_forecast))\n",
    "rmseDictionary[\"Simple Average\"] = rmse\n",
    "\n",
    "print(\"The RMS for the Simple Average Method is equal to {}\\n\".format(rmse))\n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg.avg_forecast)\n",
    "mapeDictionary['Simple_Average'] = mape \n",
    "\n",
    "y_hat_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4956558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The moving average forcasting method forcasts according the average of a number of units.  In this case we use 7 days \n",
    "#or one week.  More testing should be done to discover the best number of days to use for average\n",
    "#This should also shift by values.  Will revisit this \n",
    "\n",
    "#y_hat_avg = test.copy()\n",
    "y_hat_avg['moving_avg_forecast'] = allData['sales'].rolling(3).mean()\n",
    "y_hat_avg['moving_avg_forecast'][0] = allData['sales'][0].copy()\n",
    "y_hat_avg['moving_avg_forecast'][1] = allData['sales'][1].copy()\n",
    "y_hat_avg['moving_avg_forecast'][2] = allData['sales'][2].copy()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "#plt.plot(historic['sales'], label='Historic')\n",
    "#plt.plot(test['sales'], label='Test')\n",
    "plt.plot(allData['sales'], label='Actual Sales')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg.moving_avg_forecast))\n",
    "rmseDictionary[\"Moving Average\"] = rmse\n",
    "print(\"The RMSE for the Moving Average Method is equal to {}\\n\".format(rmse))\n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg.moving_avg_forecast)\n",
    "mapeDictionary['Moving Average'] = mape \n",
    "\n",
    "y_hat_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleExpSmoothing(np.asarray(allData['sales']))\n",
    "fit1 = model.fit()\n",
    "fit2 = model.fit(smoothing_level=0.2)\n",
    "fit3 = model.fit(smoothing_level=0.5)\n",
    "fit4 = model.fit(optimized=True)\n",
    "\n",
    "\n",
    "y_hat_avg['Simple_Exponential_Smoothing_alpha=.3'] = fit1.fittedvalues\n",
    "y_hat_avg['Simple_Exponential_Smoothing_alpha=.2'] = fit2.fittedvalues\n",
    "y_hat_avg['Simple_Exponential_Smoothing_alpha=.5'] = fit3.fittedvalues\n",
    "y_hat_avg['Simple_Exponential_Smoothing_alpha_Optimum'] = fit4.fittedvalues\n",
    "\n",
    "y_hat_avg.head()\n",
    "#print(fit1.fittedvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Exponential Smoothing Forcasting  I know that i've implemented it correctly, but i do not understand how it works \n",
    "# in python.  Need to study \n",
    "\n",
    "#y_hat_avg = test.copy()\n",
    "#y_hat_avg['SES'] = fit2.forecast(len(test))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(allData['sales'], label='Actual Sales')\n",
    "#plt.plot(test['sales'], label='Test')\n",
    "plt.plot(y_hat_avg['Simple_Exponential_Smoothing_alpha=.3'], label='SES.3')\n",
    "plt.plot(y_hat_avg['Simple_Exponential_Smoothing_alpha=.2'], label='SES.2')\n",
    "plt.plot(y_hat_avg['Simple_Exponential_Smoothing_alpha=.5'], label='SES.5')\n",
    "plt.plot(y_hat_avg['Simple_Exponential_Smoothing_alpha_Optimum'], label='SES_Optimum')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.3']))\n",
    "rmseDictionary[\"Exponential_Smoothing.3\"] = rmse\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.2']))\n",
    "rmseDictionary[\"Exponential_Smoothing.2\"] = rmse\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.5']))\n",
    "rmseDictionary[\"Exponential_Smoothing.5\"] = rmse\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha_Optimum']))\n",
    "rmseDictionary[\"Exponential_Smoothing_Optimum\"] = rmse\n",
    "\n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.3'])\n",
    "mapeDictionary['Simple_Exponential_Smoothing.3'] = mape \n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.2'])\n",
    "mapeDictionary['Simple_Exponential_Smoothing.2'] = mape \n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha=.5'])\n",
    "mapeDictionary['Simple_Exponential_Smoothing.5'] = mape \n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Simple_Exponential_Smoothing_alpha_Optimum'])\n",
    "mapeDictionary['Simple_Exponential_Smoothing_Optimum'] = mape \n",
    "\n",
    "\n",
    "#y_hat_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd01ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests Data for trends, seasonality, etc to preprocess for Holt Winter\n",
    "\n",
    "sm.tsa.seasonal_decompose(allData.sales).plot()\n",
    "result = sm.tsa.stattools.adfuller(allData.sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Holt Winter method forcasts according to trend, season, and means.  The data under consideration does not have a\n",
    "#trend.  \n",
    "\n",
    "\n",
    "#y_hat_avg = test.copy()\n",
    "model = ExponentialSmoothing(np.asarray(allData['sales']) ,seasonal_periods=7 ,trend=None, seasonal='add')\n",
    "fit1 = model.fit(optimized = True)\n",
    "fit2 = model.fit(smoothing_level=.5, smoothing_slope=None, smoothing_seasonal=.5)\n",
    "fit3 = model.fit(smoothing_level=.3, smoothing_slope=None, smoothing_seasonal=.3)\n",
    "\n",
    "\n",
    "y_hat_avg['Holt_Winter_Optimum'] = fit1.forecast(len(allData))\n",
    "y_hat_avg['Holt_Winter_.5'] = fit2.forecast(len(allData))\n",
    "y_hat_avg['Holt_Winter_.3'] = fit3.forecast(len(allData))\n",
    "\n",
    "#y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(allData['sales'], label='Actual Sales')\n",
    "\n",
    "plt.plot(y_hat_avg['Holt_Winter_Optimum'], label='Holt Winter Optimized')\n",
    "plt.plot(y_hat_avg['Holt_Winter_.5'], label='Holt Winter .5')\n",
    "plt.plot(y_hat_avg['Holt_Winter_.3'], label='Holt Winter .3')\n",
    "          \n",
    "#plt.plot( historic['sales'], label='Historic')\n",
    "#plt.plot(test['sales'], label='Test')\n",
    "#plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "   \n",
    "    \n",
    "    \n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Holt_Winter_Optimum']))\n",
    "rmseDictionary[\"Holt_Winter_Optimum\"] = rmse\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Holt_Winter_.5']))\n",
    "rmseDictionary[\"Holt_Winter_.5\"] = rmse\n",
    "\n",
    "rmse = sqrt(mean_squared_error(allData.sales, y_hat_avg['Holt_Winter_.3']))\n",
    "rmseDictionary[\"Holt_Winter_.3\"] = rmse\n",
    "    \n",
    "#rmse = sqrt(mean_squared_error(test.sales, y_hat_avg.Holt_Winter))\n",
    "\n",
    "#rmseDictionary[\"Holt Winter\"] = rmse\n",
    "\n",
    "#print(\"The RMSE for the Holt Winter Model is equal to {}\\n\".format(rmse))\n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg.Holt_Winter_Optimum)\n",
    "mapeDictionary['Holt_Winter_Optimum'] = mape \n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Holt_Winter_.5'])\n",
    "mapeDictionary['Holt_Winter_.5'] = mape \n",
    "\n",
    "mape = mape_vectorized_v2(allData.sales, y_hat_avg['Holt_Winter_.3'])\n",
    "mapeDictionary['Holt_Winter_.3'] = mape \n",
    "\n",
    "y_hat_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the Sarina Model is another seasonal model. I don't know how it works exactly.  I need to review the math and the\n",
    "#documentation.  I am getting a convergence error.  Will fix immediatly\n",
    "\n",
    "y_hat_avg = test.copy()\n",
    "fit1 = sm.tsa.statespace.SARIMAX(historic.sales, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit()\n",
    "y_hat_avg['SARIMA'] = fit1.forecast(len(test.sales), dynamic=True)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot( historic['sales'], label='Historic')\n",
    "plt.plot(test['sales'], label='Test')\n",
    "plt.plot(y_hat_avg['SARIMA'], label='SARIMA')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(test.sales, y_hat_avg.SARIMA))\n",
    "rmseDictionary[\"SARIMA\"] = rmse\n",
    "\n",
    "mape = mape_vectorized_v2(test.sales, y_hat_avg['SARIMA'])\n",
    "mapeDictionary['SARIMA'] = mape \n",
    "\n",
    "#print(\"The RMSE for the SARIMA Model is equal to {}\\n\".format(rmse))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmseDictionary)\n",
    "\n",
    "mn = min(rmseDictionary.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"The Best Model is {}\".format(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mapeDictionary)\n",
    "\n",
    "mn = min(mapeDictionary.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"The Best Model is {}\".format(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bfe90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "source_map": [
   12,
   16,
   31,
   37,
   49,
   78,
   87,
   110,
   135,
   160,
   189,
   206,
   252,
   260,
   320,
   344,
   351,
   358
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}